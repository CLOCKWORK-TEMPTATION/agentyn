#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ø®Ø¯Ù…Ø© Python Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ù†Ø¸Ø§Ù… Multi-Agent
Python Brain Service for Three-Read Breakdown System

ØªØ¯Ù…Ø¬ Ù…Ø¹ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©:
- Revolutionary Breakdown Engine
- Ultimate Breakdown System

ÙˆØªÙˆÙØ± ÙˆØ§Ø¬Ù‡Ø© FastAPI Ù„Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ TypeScript
"""

import asyncio
import uuid
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Union, Literal
from enum import Enum
from dataclasses import dataclass, asdict
import json
import traceback

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
try:
    from revolutionary_breakdown_engine import (
        MasterRevolutionarySystem,
        AdvancedSceneData
    )
    REVOLUTIONARY_AVAILABLE = True
except ImportError:
    REVOLUTIONARY_AVAILABLE = False
    logging.warning("Revolutionary Breakdown Engine ØºÙŠØ± Ù…ØªØ§Ø­")

try:
    from ultimate_breakdown_system import (
        RevolutionarySceneParser,
        DetailedBreakdown,
        split_scenes
    )
    ULTIMATE_AVAILABLE = True
except ImportError:
    ULTIMATE_AVAILABLE = False
    logging.warning("Ultimate Breakdown System ØºÙŠØ± Ù…ØªØ§Ø­")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("PythonBrainService")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Models)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ProcessingComponent(str, Enum):
    SEMANTIC_SYNOPSIS = "semantic_synopsis"
    PROP_CLASSIFICATION = "prop_classification" 
    WARDROBE_INFERENCE = "wardrobe_inference"
    CINEMATIC_PATTERNS = "cinematic_patterns"
    SCENE_SALIENCE = "scene_salience"
    CONTINUITY_CHECK = "continuity_check"
    REVOLUTIONARY_ANALYSIS = "revolutionary_analysis"
    ULTIMATE_BREAKDOWN = "ultimate_breakdown"

class AdvancedAnalysisRequest(BaseModel):
    text: str = Field(..., description="Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡")
    component: ProcessingComponent = Field(..., description="Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
    context: Optional[Dict[str, Any]] = Field(None, description="Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ")
    scene_id: Optional[str] = Field(None, description="Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø´Ù‡Ø¯")
    confidence_threshold: float = Field(0.7, description="Ø­Ø¯ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨")

class Evidence(BaseModel):
    span_start: int
    span_end: int
    text_excerpt: str
    rationale: str
    confidence: float

class AdvancedAnalysisResponse(BaseModel):
    job_id: str
    result: Dict[str, Any]
    evidence: List[Evidence]
    confidence: float
    processing_time: float
    component_version: str
    metadata: Dict[str, Any]

class JobStatus(BaseModel):
    job_id: str
    status: Literal["pending", "processing", "completed", "failed"]
    progress: float
    result: Optional[AdvancedAnalysisResponse] = None
    error: Optional[str] = None
    created_at: datetime
    updated_at: datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ø¯ÙŠØ± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (Job Manager)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class JobManager:
    def __init__(self):
        self.jobs: Dict[str, JobStatus] = {}
        self.max_jobs = 100  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
    
    def create_job(self) -> str:
        job_id = str(uuid.uuid4())
        now = datetime.now()
        
        self.jobs[job_id] = JobStatus(
            job_id=job_id,
            status="pending",
            progress=0.0,
            created_at=now,
            updated_at=now
        )
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
        self._cleanup_old_jobs()
        
        return job_id
    
    def update_job(self, job_id: str, **kwargs):
        if job_id in self.jobs:
            job = self.jobs[job_id]
            for key, value in kwargs.items():
                if hasattr(job, key):
                    setattr(job, key, value)
            job.updated_at = datetime.now()
    
    def get_job(self, job_id: str) -> Optional[JobStatus]:
        return self.jobs.get(job_id)
    
    def _cleanup_old_jobs(self):
        if len(self.jobs) > self.max_jobs:
            # Ø­Ø°Ù Ø£Ù‚Ø¯Ù… Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
            sorted_jobs = sorted(
                self.jobs.items(),
                key=lambda x: x[1].created_at
            )
            
            for job_id, _ in sorted_jobs[:len(self.jobs) - self.max_jobs]:
                del self.jobs[job_id]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª (Component Processors)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ComponentProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ù…ÙƒÙˆÙ†Ø§Øª"""
    
    def __init__(self):
        self.revolutionary_system = None
        self.ultimate_parser = None
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ØªØ§Ø­Ø©
        if REVOLUTIONARY_AVAILABLE:
            try:
                self.revolutionary_system = MasterRevolutionarySystem()
                logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Revolutionary System")
            except Exception as e:
                logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Revolutionary System: {e}")
        
        if ULTIMATE_AVAILABLE:
            try:
                self.ultimate_parser = RevolutionarySceneParser()
                logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ultimate Parser")
            except Exception as e:
                logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ultimate Parser: {e}")
    
    async def process_semantic_synopsis(self, request: AdvancedAnalysisRequest) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„Ù†Øµ"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ultimate System Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if self.ultimate_parser:
                scenes_data = split_scenes(request.text)
                if scenes_data:
                    scene_num, scene_text = scenes_data[0]  # Ø£ÙˆÙ„ Ù…Ø´Ù‡Ø¯
                    breakdown = await self.ultimate_parser.analyze_scene(scene_text, scene_num)
                    
                    return {
                        "synopsis": breakdown.summary,
                        "scene_type": str(breakdown.scene_type),
                        "characters": breakdown.cast,
                        "location": breakdown.location,
                        "time": breakdown.day_night,
                        "source": "ultimate_system"
                    }
            
            # Fallback: ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ·
            return await self._fallback_synopsis(request.text)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {e}")
            return await self._fallback_synopsis(request.text)
    
    async def process_prop_classification(self, request: AdvancedAnalysisRequest) -> Dict[str, Any]:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ø¹Ø§Ø¦Ù… ÙˆØ§Ù„Ø¹Ù†Ø§ØµØ±"""
        try:
            if self.ultimate_parser:
                scenes_data = split_scenes(request.text)
                if scenes_data:
                    scene_num, scene_text = scenes_data[0]
                    breakdown = await self.ultimate_parser.analyze_scene(scene_text, scene_num)
                    
                    return {
                        "props": breakdown.props_list,
                        "props_html": breakdown.props_html,
                        "set_dressing": breakdown.set_dressing_html,
                        "vehicles": breakdown.vehicles,
                        "classification_method": "ultimate_system",
                        "confidence": 0.85
                    }
            
            return await self._fallback_prop_classification(request.text)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¯Ø¹Ø§Ø¦Ù…: {e}")
            return await self._fallback_prop_classification(request.text)
    
    async def process_wardrobe_inference(self, request: AdvancedAnalysisRequest) -> Dict[str, Any]:
        """Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø£Ø²ÙŠØ§Ø¡"""
        try:
            if self.ultimate_parser:
                scenes_data = split_scenes(request.text)
                if scenes_data:
                    scene_num, scene_text = scenes_data[0]
                    breakdown = await self.ultimate_parser.analyze_scene(scene_text, scene_num)
                    
                    wardrobe_items = []
                    for spec in breakdown.wardrobe_specs:
                        wardrobe_items.append({
                            "character": spec.character,
                            "description": spec.description,
                            "is_inferred": spec.is_inferred,
                            "continuity_note": spec.continuity_note
                        })
                    
                    return {
                        "wardrobe_specs": wardrobe_items,
                        "costumes_html": breakdown.costumes_html,
                        "makeup_html": breakdown.makeup_html,
                        "inference_method": "ultimate_system"
                    }
            
            return await self._fallback_wardrobe_inference(request.text)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ø£Ø²ÙŠØ§Ø¡: {e}")
            return await self._fallback_wardrobe_inference(request.text)
    
    async def process_cinematic_patterns(self, request: AdvancedAnalysisRequest) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠØ©"""
        try:
            if self.ultimate_parser:
                scenes_data = split_scenes(request.text)
                if scenes_data:
                    scene_num, scene_text = scenes_data[0]
                    breakdown = await self.ultimate_parser.analyze_scene(scene_text, scene_num)
                    
                    return {
                        "cinematic_notes": breakdown.cinematic_notes,
                        "camera_lighting": breakdown.camera_lighting,
                        "production_notes": breakdown.production_notes_html,
                        "scene_type": str(breakdown.scene_type),
                        "analysis_method": "ultimate_system"
                    }
            
            return await self._fallback_cinematic_analysis(request.text)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠØ©: {e}")
            return await self._fallback_cinematic_analysis(request.text)
    
    async def process_continuity_check(self, request: AdvancedAnalysisRequest) -> Dict[str, Any]:
        """ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©"""
        try:
            if self.ultimate_parser:
                scenes_data = split_scenes(request.text)
                continuity_issues = []
                
                for i, (scene_num, scene_text) in enumerate(scenes_data):
                    breakdown = await self.ultimate_parser.analyze_scene(scene_text, scene_num)
                    
                    if breakdown.continuity_notes:
                        continuity_issues.extend([
                            {
                                "scene": scene_num,
                                "issue": note,
                                "type": "continuity"
                            }
                            for note in breakdown.continuity_notes
                        ])
                
                return {
                    "validation_passed": len(continuity_issues) == 0,
                    "issues": continuity_issues,
                    "total_scenes_checked": len(scenes_data),
                    "check_method": "ultimate_system"
                }
            
            return await self._fallback_continuity_check(request.text)
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©: {e}")
            return await self._fallback_continuity_check(request.text)
    
    async def process_revolutionary_analysis(self, request: AdvancedAnalysisRequest) -> Dict[str, Any]:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        try:
            if self.revolutionary_system and REVOLUTIONARY_AVAILABLE:
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù…Ø´Ø§Ù‡Ø¯
                scenes_data = split_scenes(request.text)
                advanced_scenes = []
                
                for scene_num, scene_text in scenes_data:
                    # Ø¥Ù†Ø´Ø§Ø¡ AdvancedSceneData
                    scene = AdvancedSceneData(scene_number=scene_num)
                    scene.original_text = scene_text
                    advanced_scenes.append(scene)
                
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ
                processed_scenes = await self.revolutionary_system.process_complete_analysis(advanced_scenes)
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                results = []
                for scene in processed_scenes:
                    results.append({
                        "scene_number": scene.scene_number,
                        "ai_confidence": scene.ai_confidence,
                        "success_probability": scene.success_probability,
                        "quantum_advantage": scene.quantum_state.quantum_advantage if scene.quantum_state else 0,
                        "neuromorphic_activation": scene.neuromorphic_activation,
                        "consciousness_level": scene.consciousness_level,
                        "creative_alternatives": scene.creative_alternatives,
                        "audience_reactions": scene.audience_reactions
                    })
                
                return {
                    "revolutionary_results": results,
                    "total_scenes": len(processed_scenes),
                    "avg_confidence": sum(s.ai_confidence for s in processed_scenes) / len(processed_scenes),
                    "analysis_method": "revolutionary_system"
                }
            
            return {"error": "Revolutionary System ØºÙŠØ± Ù…ØªØ§Ø­", "fallback_used": True}
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ: {e}")
            return {"error": str(e), "fallback_used": True}
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fallback Methods
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def _fallback_synopsis(self, text: str) -> Dict[str, Any]:
        """Ù…Ù„Ø®Øµ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¨Ø³ÙŠØ·"""
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø© ÙˆØµÙÙŠØ©
        description_lines = []
        for line in lines[1:]:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
            if ':' not in line and len(line) > 20:
                description_lines.append(line)
                if len(' '.join(description_lines)) > 150:
                    break
        
        synopsis = ' '.join(description_lines)
        if len(synopsis) > 200:
            synopsis = synopsis[:197] + '...'
        
        return {
            "synopsis": synopsis or "Ù…Ù„Ø®Øµ ØºÙŠØ± Ù…ØªÙˆÙØ±",
            "scene_type": "transition",
            "characters": [],
            "location": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
            "time": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
            "source": "fallback"
        }
    
    async def _fallback_prop_classification(self, text: str) -> Dict[str, Any]:
        """ØªØµÙ†ÙŠÙ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø¯Ø¹Ø§Ø¦Ù…"""
        text_lower = text.lower()
        
        basic_props = []
        prop_keywords = [
            'Ø¸Ø±Ù', 'Ù‡Ø§ØªÙ', 'Ù…ÙˆØ¨Ø§ÙŠÙ„', 'Ù„Ø§Ø¨ØªÙˆØ¨', 'Ø­Ø§Ø³Ø¨', 'Ù…Ø¬Ù„Ø©', 
            'Ø­Ù‚ÙŠØ¨Ø©', 'ÙƒØ£Ø³', 'ÙƒÙˆØ¨', 'Ù…ÙØªØ§Ø­', 'Ù†Ø¸Ø§Ø±Ø©', 'Ø³Ø§Ø¹Ø©'
        ]
        
        for keyword in prop_keywords:
            if keyword in text_lower:
                basic_props.append(keyword)
        
        return {
            "props": basic_props,
            "props_html": ', '.join(basic_props) if basic_props else 'Ù„Ø§ ÙŠÙˆØ¬Ø¯',
            "set_dressing": "Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ø¹",
            "vehicles": "Ù„Ø§ ÙŠÙˆØ¬Ø¯",
            "classification_method": "fallback",
            "confidence": 0.6
        }
    
    async def _fallback_wardrobe_inference(self, text: str) -> Dict[str, Any]:
        """Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø£Ø²ÙŠØ§Ø¡"""
        return {
            "wardrobe_specs": [
                {
                    "character": "Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
                    "description": "Ù…Ù„Ø§Ø¨Ø³ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø´Ù‡Ø¯",
                    "is_inferred": True,
                    "continuity_note": ""
                }
            ],
            "costumes_html": "Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚",
            "makeup_html": "ØªØµØ­ÙŠØ­ ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ø¹ØªÙŠØ§Ø¯ÙŠ",
            "inference_method": "fallback"
        }
    
    async def _fallback_cinematic_analysis(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠØ©"""
        return {
            "cinematic_notes": "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø±Ø§ÙƒÙˆØ±Ø§Øª (Continuity)",
            "camera_lighting": "Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ù„ÙˆÙ‚Øª",
            "production_notes": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø®Ø§ØµØ©",
            "scene_type": "transition",
            "analysis_method": "fallback"
        }
    
    async def _fallback_continuity_check(self, text: str) -> Dict[str, Any]:
        """ÙØ­Øµ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ©"""
        return {
            "validation_passed": True,
            "issues": [],
            "total_scenes_checked": 1,
            "check_method": "fallback"
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªØ·Ø¨ÙŠÙ‚ FastAPI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(
    title="Python Brain Service",
    description="Ø®Ø¯Ù…Ø© Python Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ù†Ø¸Ø§Ù… Multi-Agent Ù„Ù„ØªÙØ±ÙŠØº Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ",
    version="1.0.0"
)

# Ø¥Ø¹Ø¯Ø§Ø¯ CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
job_manager = JobManager()
processor = ComponentProcessor()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoints)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
async def root():
    return {
        "service": "Python Brain Service",
        "version": "1.0.0",
        "status": "running",
        "available_systems": {
            "revolutionary": REVOLUTIONARY_AVAILABLE,
            "ultimate": ULTIMATE_AVAILABLE
        }
    }

@app.post("/analyze/async")
async def start_analysis(request: AdvancedAnalysisRequest, background_tasks: BackgroundTasks):
    """Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†"""
    job_id = job_manager.create_job()
    
    background_tasks.add_task(process_analysis, job_id, request)
    
    return {
        "job_id": job_id,
        "status": "started",
        "message": "ØªÙ… Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„"
    }

@app.get("/jobs/{job_id}")
async def get_job_status(job_id: str):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„ÙˆØ¸ÙŠÙØ©"""
    job = job_manager.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Ø§Ù„ÙˆØ¸ÙŠÙØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")
    
    return job

@app.get("/jobs")
async def list_jobs():
    """Ù‚Ø§Ø¦Ù…Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù"""
    return {
        "jobs": list(job_manager.jobs.values()),
        "total": len(job_manager.jobs)
    }

@app.post("/analyze/sync")
async def analyze_sync(request: AdvancedAnalysisRequest):
    """ØªØ­Ù„ÙŠÙ„ Ù…ØªØ²Ø§Ù…Ù† (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)"""
    try:
        start_time = datetime.now()
        
        result = await process_component(request)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return AdvancedAnalysisResponse(
            job_id=f"sync_{uuid.uuid4()}",
            result=result,
            evidence=[],
            confidence=result.get("confidence", 0.8),
            processing_time=processing_time,
            component_version="1.0.0",
            metadata={
                "component": request.component,
                "sync_mode": True
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¸Ø§Ø¦Ù (Job Processing)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def process_analysis(job_id: str, request: AdvancedAnalysisRequest):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    try:
        job_manager.update_job(job_id, status="processing", progress=0.1)
        
        start_time = datetime.now()
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        result = await process_component(request)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        response = AdvancedAnalysisResponse(
            job_id=job_id,
            result=result,
            evidence=extract_evidence(request.text, result),
            confidence=result.get("confidence", 0.8),
            processing_time=processing_time,
            component_version="1.0.0",
            metadata={
                "component": request.component,
                "scene_id": request.scene_id,
                "context": request.context
            }
        )
        
        job_manager.update_job(
            job_id,
            status="completed",
            progress=1.0,
            result=response
        )
        
        logger.info(f"âœ… ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ÙˆØ¸ÙŠÙØ© {job_id} ÙÙŠ {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
        
    except Exception as e:
        error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¸ÙŠÙØ©: {str(e)}"
        logger.error(f"âŒ {error_msg}\n{traceback.format_exc()}")
        
        job_manager.update_job(
            job_id,
            status="failed",
            error=error_msg
        )

async def process_component(request: AdvancedAnalysisRequest) -> Dict[str, Any]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙƒÙˆÙ† Ø§Ù„Ù…Ø­Ø¯Ø¯"""
    
    if request.component == ProcessingComponent.SEMANTIC_SYNOPSIS:
        return await processor.process_semantic_synopsis(request)
    
    elif request.component == ProcessingComponent.PROP_CLASSIFICATION:
        return await processor.process_prop_classification(request)
    
    elif request.component == ProcessingComponent.WARDROBE_INFERENCE:
        return await processor.process_wardrobe_inference(request)
    
    elif request.component == ProcessingComponent.CINEMATIC_PATTERNS:
        return await processor.process_cinematic_patterns(request)
    
    elif request.component == ProcessingComponent.CONTINUITY_CHECK:
        return await processor.process_continuity_check(request)
    
    elif request.component == ProcessingComponent.REVOLUTIONARY_ANALYSIS:
        return await processor.process_revolutionary_analysis(request)
    
    else:
        raise ValueError(f"Ù…ÙƒÙˆÙ† ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {request.component}")

def extract_evidence(text: str, result: Dict[str, Any]) -> List[Evidence]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¯Ù„Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    evidence = []
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø¯Ù„Ø© Ø¨Ø³ÙŠØ·Ø©
    if "props" in result:
        for prop in result["props"]:
            if prop in text:
                start_pos = text.find(prop)
                evidence.append(Evidence(
                    span_start=start_pos,
                    span_end=start_pos + len(prop),
                    text_excerpt=prop,
                    rationale=f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø¹Ù…Ø©: {prop}",
                    confidence=0.8
                ))
    
    return evidence

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Python Brain Service...")
    logger.info(f"Revolutionary System: {'âœ… Ù…ØªØ§Ø­' if REVOLUTIONARY_AVAILABLE else 'âŒ ØºÙŠØ± Ù…ØªØ§Ø­'}")
    logger.info(f"Ultimate System: {'âœ… Ù…ØªØ§Ø­' if ULTIMATE_AVAILABLE else 'âŒ ØºÙŠØ± Ù…ØªØ§Ø­'}")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )